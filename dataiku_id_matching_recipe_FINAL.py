# -*- coding: utf-8 -*-
"""
Dataiku Python Recipe: ID Matching & Assignment (VERSION FINALE)
=================================================================

CORRECTIONS APPLIQU√âES:
- Pr√©servation de toutes les colonnes du fichier source
- Validation des colonnes requises au d√©marrage
- Gestion correcte des doublons de hash (garder le premier)
- Normalisation sans perte de donn√©es
- V√©rifications d'int√©grit√© √©tendues
- ID en premi√®re colonne pour Supabase
"""

import dataiku
import pandas as pd
import hashlib
import uuid
from datetime import datetime

# ============================================================================
# CONFIGURATION
# ============================================================================

NATURAL_KEY = [
    'Nom',
    'P√©rim√®tre', 
    'Localisation',
    'Source',
    'Date',
    'Unit√© donn√©e d\'activit√©'  # AJOUT√â: Diff√©rencie les records par unit√© (kg vs m¬≤)
]

MONITORED_COLUMNS = [
    'FE',
    'Incertitude',
    'Commentaires',
    'Description',
    'Unit√© donn√©e d\'activit√©'
]

# ============================================================================
# FONCTION DE NORMALISATION
# ============================================================================

def normalize_dataframe(df):
    """
    Normalise les valeurs du dataframe pour √©viter les faux positifs.
    IMPORTANT: Travaille sur une copie pour ne pas modifier le DataFrame original.
    """
    df_normalized = df.copy()
    
    # Normaliser les colonnes texte (uniquement pour la comparaison)
    text_columns = df_normalized.select_dtypes(include=['object']).columns
    
    for col in text_columns:
        if col in df_normalized.columns:
            # G√©rer les NaN avant conversion pour √©viter 'nan' string
            df_normalized[col] = df_normalized[col].fillna('').astype(str).str.strip()
            
            # Remplacer multiples espaces par un seul
            df_normalized[col] = df_normalized[col].str.replace(r'\s+', ' ', regex=True)
    
    # Normaliser les nombres (arrondir pour √©viter diff√©rences de pr√©cision)
    numeric_columns = df_normalized.select_dtypes(include=['float64', 'float32', 'int64', 'int32']).columns
    
    for col in numeric_columns:
        if col in df_normalized.columns:
            # Arrondir √† 6 d√©cimales
            df_normalized[col] = df_normalized[col].round(6)
    
    return df_normalized

# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def generate_natural_key_hash(row, key_columns):
    """G√©n√®re un hash stable bas√© sur la cl√© naturelle."""
    key_values = [str(row[col]).strip() if pd.notna(row[col]) else '' 
                  for col in key_columns]
    key_string = '|'.join(key_values)
    return hashlib.sha256(key_string.encode('utf-8')).hexdigest()[:16]

def generate_new_uuid():
    """G√©n√®re un nouvel UUID pour les nouveaux records"""
    return str(uuid.uuid4())

def compare_records(old_row, new_row, columns):
    """Compare deux records sur les colonnes sp√©cifi√©es."""
    has_changes = False
    changes_dict = {}
    
    for col in columns:
        if col not in new_row.index or col not in old_row.index:
            continue
            
        old_val = old_row[col]
        new_val = new_row[col]
        
        # G√©rer les NaN
        if pd.isna(old_val) and pd.isna(new_val):
            continue
        
        # Comparer les valeurs
        if old_val != new_val:
            has_changes = True
            changes_dict[col] = {
                'old': old_val,
                'new': new_val
            }
    
    return has_changes, changes_dict

# ============================================================================
# √âTAPE 1: CHARGER LES DATASETS
# ============================================================================

print("=" * 80)
print("√âTAPE 1: CHARGEMENT DES DONN√âES")
print("=" * 80)

# Nouveau CSV import
df_new = dataiku.Dataset("importaxelsupraadmin_prepared").get_dataframe()
print(f"‚úì Nouveau import charg√©: {len(df_new):,} records")

# Forcer les colonnes texte √† rester en type 'object' (string)
# Cela √©vite que Pandas les convertisse en float si elles sont majoritairement vides
text_columns_to_force = [
    'M√©thodologie', 'M√©thodologie_en',
    'Commentaires', 'Commentaires_en',
    'Description', 'Description_en',
    'Nom', 'Nom_en',
    'P√©rim√®tre', 'P√©rim√®tre_en',
    'Secteur', 'Secteur_en',
    'Sous-secteur', 'Sous-secteur_en',
    'Localisation', 'Localisation_en',
    'Source',
    'Contributeur', 'Contributeur_en',
    'Type_de_donn√©es', 'Type_de_donn√©es_en',
    'Unit√© donn√©e d\'activit√©', 'Unite_en',
    'Incertitude'
]

for col in text_columns_to_force:
    if col in df_new.columns:
        df_new[col] = df_new[col].astype(str).replace('nan', '').replace('None', '')

print(f"‚úì Types de colonnes texte normalis√©s")

# IMPORTANT: Garder une copie non modifi√©e du df_new
df_new_original = df_new.copy()

# Validation des colonnes requises
print("\nüîç VALIDATION DES COLONNES REQUISES...")
required_cols = set(NATURAL_KEY + MONITORED_COLUMNS)
missing_in_new = required_cols - set(df_new.columns)
if missing_in_new:
    raise ValueError(f"‚ùå ERREUR: Colonnes manquantes dans l'import: {sorted(missing_in_new)}\n"
                     f"   Colonnes pr√©sentes: {sorted(df_new.columns)}")
print(f"‚úì Toutes les colonnes requises sont pr√©sentes")

# CSV source avec IDs existants
try:
    df_source = dataiku.Dataset("emission_factors_source").get_dataframe()
    print(f"‚úì Source existante charg√©e: {len(df_source):,} records")
    
    if 'ID' not in df_source.columns or len(df_source) == 0:
        print(f"‚ö†Ô∏è  Source sans colonne ID ou vide (premier import)")
        has_source = False
        df_source = pd.DataFrame()
    else:
        # Forcer les types pour la source aussi
        for col in text_columns_to_force:
            if col in df_source.columns:
                df_source[col] = df_source[col].astype(str).replace('nan', '').replace('None', '')
        print(f"‚úì Types de colonnes texte normalis√©s pour la source")
        has_source = True
        
except Exception as e:
    print(f"‚ö†Ô∏è  Pas de source existante (premier import): {e}")
    df_source = pd.DataFrame()
    has_source = False

# ============================================================================
# √âTAPE 1.5: NORMALISATION POUR COMPARAISON UNIQUEMENT
# ============================================================================

print("\n" + "=" * 80)
print("√âTAPE 1.5: NORMALISATION POUR COMPARAISON")
print("=" * 80)

print("‚öôÔ∏è  Cr√©ation de versions normalis√©es pour la comparaison...")
# On cr√©e des versions normalis√©es S√âPAR√âES pour la comparaison
df_new_normalized = normalize_dataframe(df_new)
print("‚úì Version normalis√©e du nouveau import cr√©√©e")

if has_source:
    df_source_normalized = normalize_dataframe(df_source)
    print("‚úì Version normalis√©e de la source cr√©√©e")
else:
    df_source_normalized = pd.DataFrame()

print("\nüìù Normalisation appliqu√©e (pour comparaison uniquement):")
print("  - Suppression des espaces superflus")
print("  - Uniformisation des espaces multiples")
print("  - Arrondi des nombres √† 6 d√©cimales")

# Reset index pour garantir l'alignement entre original et normalis√©
df_new_normalized = df_new_normalized.reset_index(drop=True)
df_new_original = df_new_original.reset_index(drop=True)
print("‚úì Index r√©initialis√©s pour alignement garanti")

# ============================================================================
# √âTAPE 2: G√âN√âRER LES HASHES DE CL√â NATURELLE
# ============================================================================

print("\n" + "=" * 80)
print("√âTAPE 2: G√âN√âRATION DES HASHES")
print("=" * 80)

# G√©n√©rer les hashes sur les versions normalis√©es
df_new_normalized['natural_key_hash'] = df_new_normalized.apply(
    lambda row: generate_natural_key_hash(row, NATURAL_KEY),
    axis=1
)
print(f"‚úì Hashes g√©n√©r√©s pour nouveau import")
print(f"  - Hashes uniques: {df_new_normalized['natural_key_hash'].nunique():,}")

# V√©rifier les cl√©s vides
empty_keys = df_new_normalized.apply(
    lambda row: all(not str(row[col]).strip() for col in NATURAL_KEY),
    axis=1
).sum()
if empty_keys > 0:
    print(f"‚ö†Ô∏è  ATTENTION: {empty_keys:,} records avec cl√© naturelle vide d√©tect√©s")
    print(f"   Ces records recevront des IDs uniques mais ne pourront pas √™tre match√©s")

if has_source:
    if 'natural_key_hash' not in df_source_normalized.columns:
        print(f"‚ö†Ô∏è  G√©n√©ration des hashes pour la source")
        df_source_normalized['natural_key_hash'] = df_source_normalized.apply(
            lambda row: generate_natural_key_hash(row, NATURAL_KEY),
            axis=1
        )
    print(f"‚úì Hashes disponibles pour source")
    print(f"  - Hashes uniques: {df_source_normalized['natural_key_hash'].nunique():,}")

# ============================================================================
# √âTAPE 3: MATCHING ET CLASSIFICATION
# ============================================================================

print("\n" + "=" * 80)
print("√âTAPE 3: MATCHING PAR CL√â NATURELLE")
print("=" * 80)

results = {
    'new': [],
    'update': [],
    'unchanged': []
}

if has_source:
    source_dict = {}
    hash_duplicates_count = 0
    
    for idx, row in df_source_normalized.iterrows():
        natural_hash = row['natural_key_hash']
        if natural_hash not in source_dict:
            # Garder le premier (stabilit√© des IDs)
            source_dict[natural_hash] = {
                'ID': df_source.loc[idx, 'ID'],  # Prendre l'ID depuis le df_source original
                'data': row  # Donn√©es normalis√©es pour comparaison
            }
        else:
            # Doublon d√©tect√© (m√™me hash)
            hash_duplicates_count += 1
    
    print(f"‚úì Index de lookup cr√©√©: {len(source_dict):,} records")
    if hash_duplicates_count > 0:
        print(f"‚ö†Ô∏è  {hash_duplicates_count:,} doublons de hash d√©tect√©s dans source")
        print(f"   (Premi√®re occurrence gard√©e pour stabilit√© des IDs)")
    
    for idx, new_row in df_new_normalized.iterrows():
        natural_hash = new_row['natural_key_hash']
        
        if natural_hash in source_dict:
            source_record = source_dict[natural_hash]
            existing_id = source_record['ID']
            existing_data = source_record['data']
            
            # Comparer les versions normalis√©es
            has_changes, changes = compare_records(
                existing_data, 
                new_row, 
                MONITORED_COLUMNS
            )
            
            if has_changes:
                results['update'].append({
                    'index': idx,
                    'ID': existing_id,
                    'natural_key_hash': natural_hash,
                    'changes': changes
                })
            else:
                results['unchanged'].append({
                    'index': idx,
                    'ID': existing_id,
                    'natural_key_hash': natural_hash
                })
        else:
            results['new'].append({
                'index': idx,
                'ID': generate_new_uuid(),
                'natural_key_hash': natural_hash
            })
    
    print(f"\nüìä R√âSULTATS DU MATCHING:")
    print(f"  ‚úÖ Nouveaux records: {len(results['new']):,}")
    print(f"  üîÑ Updates: {len(results['update']):,}")
    print(f"  ‚úì Inchang√©s: {len(results['unchanged']):,}")
    
    if len(results['update']) > 0 and len(results['update']) <= 10:
        print(f"\nüìù EXEMPLES D'UPDATES D√âTECT√âS:")
        for i, update in enumerate(results['update'], 1):
            idx = update['index']
            row = df_new_normalized.loc[idx]
            print(f"\n  Update #{i}:")
            print(f"    ID: {update['ID']}")
            nom_display = str(row['Nom'])[:60] if pd.notna(row['Nom']) else '(vide)'
            print(f"    Nom: {nom_display}...")
            print(f"    Changements:")
            for col, change in update['changes'].items():
                old_str = str(change['old'])[:40]
                new_str = str(change['new'])[:40]
                print(f"      {col}: '{old_str}' ‚Üí '{new_str}'")

else:
    print("‚úì Premier import - g√©n√©ration de nouveaux IDs pour tous les records")
    for idx, new_row in df_new_normalized.iterrows():
        results['new'].append({
            'index': idx,
            'ID': generate_new_uuid(),
            'natural_key_hash': new_row['natural_key_hash']
        })
    
    print(f"\nüìä R√âSULTATS:")
    print(f"  ‚úÖ Nouveaux records: {len(results['new']):,}")

# ============================================================================
# √âTAPE 4: ASSIGNER LES IDs AU DATAFRAME ORIGINAL (NON NORMALIS√â)
# ============================================================================

print("\n" + "=" * 80)
print("√âTAPE 4: ASSIGNATION DES IDs AU DATAFRAME ORIGINAL")
print("=" * 80)

id_mapping = {}
operation_mapping = {}
hash_mapping = {}

for record in results['new']:
    id_mapping[record['index']] = record['ID']
    operation_mapping[record['index']] = 'INSERT'
    hash_mapping[record['index']] = record['natural_key_hash']

for record in results['update']:
    id_mapping[record['index']] = record['ID']
    operation_mapping[record['index']] = 'UPDATE'
    hash_mapping[record['index']] = record['natural_key_hash']

for record in results['unchanged']:
    id_mapping[record['index']] = record['ID']
    operation_mapping[record['index']] = 'UNCHANGED'
    hash_mapping[record['index']] = record['natural_key_hash']

# IMPORTANT: Utiliser le dataframe ORIGINAL, pas la version normalis√©e
df_output = df_new_original.copy()

# Ajouter les colonnes de m√©tadonn√©es
df_output['ID'] = df_output.index.map(id_mapping)
df_output['natural_key_hash'] = df_output.index.map(hash_mapping)
df_output['operation'] = df_output.index.map(operation_mapping)
df_output['import_timestamp'] = datetime.now()
df_output['matched_by_natural_key'] = True

# R√©organiser les colonnes : m√©tadonn√©es d'abord, puis donn√©es
metadata_cols = ['ID', 'natural_key_hash', 'operation', 'import_timestamp', 'matched_by_natural_key']
data_cols = [col for col in df_output.columns if col not in metadata_cols]
df_output = df_output[metadata_cols + data_cols]

print(f"‚úì IDs assign√©s au dataframe original (non normalis√©)")
print(f"‚úì Colonnes r√©organis√©es (m√©tadonn√©es en premier)")

# V√©rifications
null_ids = df_output['ID'].isna().sum()
if null_ids > 0:
    print(f"‚ö†Ô∏è  WARNING: {null_ids:,} records sans ID !")
else:
    print(f"‚úì Tous les records ont un ID")

duplicate_ids = df_output['ID'].duplicated().sum()
if duplicate_ids > 0:
    print(f"‚ö†Ô∏è  WARNING: {duplicate_ids:,} IDs dupliqu√©s d√©tect√©s !")
    print(f"   Application de la d√©duplication automatique...")
    print(f"   STRAT√âGIE: Garder uniquement les records du NOUVEL IMPORT (version la plus r√©cente)")
    
    # Trier par operation pour garantir que les nouveaux imports (INSERT/UPDATE) 
    # viennent AVANT les UNCHANGED (qui viennent de la source)
    # Puis d√©dupliquer en gardant le premier = celui du nouvel import
    operation_priority = {'INSERT': 1, 'UPDATE': 2, 'UNCHANGED': 3}
    df_output['_priority'] = df_output['operation'].map(operation_priority)
    df_output = df_output.sort_values('_priority')
    df_output = df_output.drop_duplicates(subset=['ID'], keep='first')
    df_output = df_output.drop(columns=['_priority'])
    
    print(f"‚úì D√©duplication appliqu√©e: {len(df_output):,} records uniques restants")
    print(f"‚úì Priorit√© donn√©e aux records INSERT/UPDATE du nouvel import")
else:
    print(f"‚úì Tous les IDs sont uniques")

# V√âRIFICATION DES DONN√âES: Info uniquement (pas d'erreur si d√©duplication)
print("\nüîç V√âRIFICATION DE L'INT√âGRIT√â DES DONN√âES:")
print(f"  Records avant d√©duplication: {len(df_new_original):,}")
print(f"  Records apr√®s d√©duplication: {len(df_output):,}")
if len(df_output) < len(df_new_original):
    diff = len(df_new_original) - len(df_output)
    print(f"  ‚úì {diff:,} doublons supprim√©s (attendu)")
else:
    print(f"  ‚úì Aucune d√©duplication n√©cessaire")

# V√©rifier qu'aucun record n'a perdu ses donn√©es essentielles
critical_cols = ['Nom', 'Source']  # Colonnes qui ne doivent JAMAIS √™tre vides
for col in critical_cols:
    if col in df_output.columns:
        nulls = df_output[col].isna().sum()
        if nulls > 0:
            print(f"  ‚ö†Ô∏è  Colonne '{col}': {nulls:,} valeurs vides d√©tect√©es")
        else:
            print(f"  ‚úì Colonne '{col}': Toutes les valeurs pr√©sentes")

print(f"‚úì V√©rification d'int√©grit√© termin√©e")

# ============================================================================
# √âTAPE 5: STATISTIQUES FINALES
# ============================================================================

print("\n" + "=" * 80)
print("√âTAPE 5: STATISTIQUES FINALES")
print("=" * 80)

print(f"\nüìä R√âSUM√â:")
print(f"  Total records: {len(df_output):,}")
print(f"  - INSERT (nouveaux): {(df_output['operation'] == 'INSERT').sum():,}")
print(f"  - UPDATE (modifi√©s): {(df_output['operation'] == 'UPDATE').sum():,}")
print(f"  - UNCHANGED (identiques): {(df_output['operation'] == 'UNCHANGED').sum():,}")

# ============================================================================
# √âTAPE 6: √âCRIRE LE R√âSULTAT
# ============================================================================

print("\n" + "=" * 80)
print("√âTAPE 6: √âCRITURE DU R√âSULTAT")
print("=" * 80)

output = dataiku.Dataset("emission_factors_with_ids")
output.write_with_schema(df_output)

print(f"‚úì Dataset 'emission_factors_with_ids' cr√©√©")
print(f"  - {len(df_output):,} records")
print(f"  - Toutes les colonnes originales pr√©serv√©es")
print(f"  - Pr√™t pour upload Supabase")

print("\n" + "=" * 80)
print("‚úÖ TRAITEMENT TERMIN√â")
print("=" * 80)

print(f"""
PROCHAINES √âTAPES:
1. V√©rifier le dataset 'emission_factors_with_ids'
2. Comparer quelques lignes avec le fichier input pour validation
3. Si correct, uploader vers Supabase (UPSERT sur ID)
4. Sauvegarder 'emission_factors_with_ids' comme nouvelle source
""")

