# üìò Guide Complet : Syst√®me de Matching d'ID Dataiku

**Version** : 2.0 (Octobre 2025)  
**Fichier Code** : `dataiku_id_matching_recipe_FINAL.py`  
**Statut** : ‚úÖ Production-ready

---

## üéØ Objectif

Maintenir des **IDs stables** pour les facteurs d'√©mission entre diff√©rents imports admin, en utilisant une **cl√© naturelle** bas√©e sur les caract√©ristiques m√©tier des records.

### Probl√®me R√©solu

‚ùå **Avant** : Chaque import g√©n√©rait de nouveaux IDs ‚Üí Perte de tra√ßabilit√©  
‚úÖ **Apr√®s** : Les m√™mes facteurs conservent leurs IDs ‚Üí Continuit√© garantie

---

## üîë Concept : La Cl√© Naturelle

### D√©finition

Une **combinaison de colonnes** qui identifient **de mani√®re unique** un facteur d'√©mission :

```python
NATURAL_KEY = [
    'Nom',                      # Ex: "Peintures BIOPRO"
    'P√©rim√®tre',                # Ex: "A1-A2-A4-A5-C1-C2-C3-C4"
    'Localisation',             # Ex: "France"
    'Source',                   # Ex: "INIES"
    'Date',                     # Ex: 2025
    'Unit√© donn√©e d\'activit√©'  # Ex: "kg" ou "m¬≤"
]
```

### Pourquoi l'Unit√© est Cruciale ?

**Sans l'unit√©** :
```
"Peintures BIOPRO, France, INIES, 2025"
  ‚Üí M√™me hash pour kg ET m¬≤
  ‚Üí M√™me ID assign√©
  ‚Üí DOUBLON d'ID ‚ùå
```

**Avec l'unit√©** :
```
"Peintures BIOPRO, France, INIES, 2025, kg"  ‚Üí Hash A ‚Üí ID 001
"Peintures BIOPRO, France, INIES, 2025, m¬≤"  ‚Üí Hash B ‚Üí ID 002
  ‚Üí IDs diff√©rents ‚úÖ
```

---

## ‚öôÔ∏è Fonctionnement du Code

### 1. Normalisation des Donn√©es

```python
def normalize_dataframe(df):
    # Nettoyer les espaces, casse, arrondir les nombres
    # Garantit que "France" = "france " = "FRANCE"
```

**Objectif** : Deux records identiques doivent avoir le m√™me hash, m√™me avec des variations mineures de formatage.

### 2. G√©n√©ration du Hash

```python
def generate_natural_key_hash(row):
    # Concat√®ne les valeurs de NATURAL_KEY
    # G√©n√®re un hash court (16 caract√®res)
    return hashlib.blake2b(key.encode(), digest_size=8).hexdigest()
```

**Exemple** :
```
Input:  Nom="Peintures BIOPRO" + P√©rim√®tre="A1..." + ... + Unit√©="kg"
Output: Hash = "25bbddf40141fd23"
```

### 3. Matching avec la Source

```python
for idx, row in df_new_normalized.iterrows():
    natural_hash = row['natural_key_hash']
    
    if natural_hash in source_dict:
        # MATCH TROUV√â ‚Üí R√©utiliser l'ID existant
        existing_id = source_dict[natural_hash]['ID']
        
        if compare_records(row, source_dict[natural_hash]['data']):
            operation = 'UNCHANGED'  # Aucun changement
        else:
            operation = 'UPDATE'      # Valeurs modifi√©es (FE, etc.)
    else:
        # NOUVEAU RECORD ‚Üí G√©n√©rer un nouvel UUID
        existing_id = generate_new_uuid()
        operation = 'INSERT'
```

### 4. Classification des Operations

| Operation | Signification |
|-----------|---------------|
| **INSERT** | Nouveau facteur d'√©mission (hash inconnu) |
| **UPDATE** | Facteur existant avec valeurs modifi√©es (FE, Incertitude, etc.) |
| **UNCHANGED** | Facteur existant sans aucun changement |

### 5. D√©duplication Intelligente

**Si des doublons d'ID subsistent** (cas edge) :

```python
if duplicate_ids > 0:
    # Tri par priorit√©: INSERT > UPDATE > UNCHANGED
    operation_priority = {'INSERT': 1, 'UPDATE': 2, 'UNCHANGED': 3}
    df_output['_priority'] = df_output['operation'].map(operation_priority)
    df_output = df_output.sort_values('_priority')
    
    # Garder uniquement la premi√®re occurrence = nouvel import
    df_output = df_output.drop_duplicates(subset=['ID'], keep='first')
```

**Garantie** : **0 doublon d'ID** en sortie, avec priorit√© au nouvel import (version la plus r√©cente).

---

## üöÄ Utilisation dans Dataiku

### √âtape 1 : Cr√©er le Projet

1. **Nouveau projet** : "Emission Factors ID Matching"
2. **Import CSV** : Dataset "new_import" (ton fichier d'import admin)
3. **Dataset source** : "emission_factors_source" (vide au premier run)

### √âtape 2 : Cr√©er la Recette Python

1. **Recette Python** : "ID Matching"
2. **Input** : `new_import`, `emission_factors_source`
3. **Output** : `emission_factors_with_ids`

### √âtape 3 : Copier le Code

1. Ouvre `dataiku_id_matching_recipe_FINAL.py`
2. **Copie TOUT le contenu**
3. **Colle** dans la recette Dataiku (remplace tout)
4. **Enregistre** (Ctrl+S / Cmd+S)

### √âtape 4 : Premier Run (Fresh Start)

**Source vide = Tous les records en INSERT**

1. Lance la recette
2. **Logs attendus** :
   ```
   üìä STATISTIQUES FINALES:
   üÜï Insertions : 453,584
   üìù Updates     : 0
   ‚ÜîÔ∏è  Inchang√©s   : 0
   
   ‚úì Tous les IDs sont uniques
   ```

3. **Sauvegarde** : `emission_factors_with_ids` ‚Üí `emission_factors_source`

### √âtape 5 : Runs Suivants (Matching Actif)

**Source remplie = Matching des IDs existants**

1. Nouveau import ‚Üí Dataset `new_import`
2. Lance la recette
3. **Logs attendus** :
   ```
   üìä STATISTIQUES FINALES:
   üÜï Insertions : 150      ‚Üê Nouveaux facteurs
   üìù Updates     : 1,230   ‚Üê Valeurs modifi√©es
   ‚ÜîÔ∏è  Inchang√©s   : 452,204 ‚Üê Aucun changement
   
   ‚úì Tous les IDs sont uniques
   ```

4. **Sauvegarde** : Mettre √† jour `emission_factors_source`

---

## ‚úÖ Garanties du Syst√®me

### 1. Unicit√© des IDs : 100%

**Ligne 415 du code** garantit math√©matiquement :
```python
df_output = df_output.drop_duplicates(subset=['ID'], keep='first')
```

‚Üí **Impossible d'avoir un doublon d'ID** en sortie

### 2. Priorit√© au Nouvel Import

En cas de doublon (cas edge), **toujours garder la version la plus r√©cente** :
- INSERT (priorit√© 1) > UPDATE (priorit√© 2) > UNCHANGED (priorit√© 3)

### 3. Stabilit√© des IDs

**Tant que la cl√© naturelle ne change pas**, l'ID reste identique :
- Modification du FE ‚Üí M√™me ID (operation = UPDATE)
- Modification de l'Unit√© ‚Üí Nouvel ID (nouvelle cl√© naturelle)

### 4. Int√©grit√© des Donn√©es

**Colonnes essentielles v√©rifi√©es** :
```python
['Nom', 'Source']  # Ne doivent JAMAIS √™tre vides
```

‚Üí Si valeurs vides d√©tect√©es ‚Üí **Alerte dans les logs** (pas d'erreur bloquante)

**Note** : La d√©duplication peut r√©duire le nombre total de records (suppression volontaire des doublons d'ID), ce qui est normal et attendu.

---

## üîç Validation Post-Ex√©cution

### Dans Dataiku (Logs)

```
‚úÖ √Ä v√©rifier :
‚úì Tous les records ont un ID
‚úì Tous les IDs sont uniques
‚úì Int√©grit√© v√©rifi√©e pour 7 colonnes critiques
```

### Dans Supabase (SQL)

Apr√®s upload, ex√©cute `scripts/validate_no_duplicate_ids.sql` :

```sql
-- Test 1: Compter les doublons (doit retourner 0 rows)
SELECT "ID", COUNT(*) 
FROM staging_emission_factors
GROUP BY "ID"
HAVING COUNT(*) > 1;

-- Test 2: V√©rifier la coh√©rence
SELECT 
    COUNT(*) as total,
    COUNT(DISTINCT "ID") as unique_ids
FROM staging_emission_factors;
-- total = unique_ids ‚úÖ
```

---

## üõ†Ô∏è R√©solution de Probl√®mes

### Probl√®me 1 : Doublons d'ID D√©tect√©s

**Cause** : La colonne Unit√© n'est pas dans `NATURAL_KEY`

**Solution** : V√©rifier ligne 31 du code :
```python
'Unit√© donn√©e d\'activit√©'  # Doit √™tre pr√©sent !
```

### Probl√®me 2 : Anciennes Valeurs Gard√©es

**Cause** : Tri par priorit√© non actif

**Solution** : V√©rifier lignes 412-414 :
```python
operation_priority = {'INSERT': 1, 'UPDATE': 2, 'UNCHANGED': 3}
```

### Probl√®me 3 : Erreur "Colonnes Manquantes"

**Cause** : Nom de colonne diff√©rent dans le CSV

**Solution** : Ajuster `NATURAL_KEY` avec les vrais noms de colonnes

### Probl√®me 4 : "M√©thodologie" en Float au lieu de String

**Cause** : Pandas inf√®re le type automatiquement

**Solution** : ‚úÖ D√©j√† corrig√© (lignes 124-148) - For√ßage explicite en `str`

---

## üìä Statistiques Typiques

### Premier Run (Fresh Start)

```
Input:  453,584 records
Source: 0 records

Output: 453,584 records
  - INSERT: 453,584 (100%)
  - UPDATE: 0
  - UNCHANGED: 0

IDs uniques: 453,584 ‚úÖ
Doublons: 0 ‚úÖ
```

### Run Suivant (Matching Actif)

```
Input:  453,734 records (+150)
Source: 453,584 records

Output: 453,734 records
  - INSERT: 150 (nouveaux)
  - UPDATE: 1,230 (valeurs modifi√©es)
  - UNCHANGED: 452,354 (inchang√©s)

IDs uniques: 453,734 ‚úÖ
Doublons: 0 ‚úÖ
```

---

## üéì Cas d'Usage R√©els

### Cas 1 : Nouveau Produit

```
Input:
  - Nom: "B√©ton C35/45"
  - Unit√©: "m¬≥"
  - Source: "INIES"
  - Date: 2025
  - FE: 350.5

R√©sultat:
  - Hash g√©n√©r√©: abc123...
  - Aucun match dans source
  - Operation: INSERT
  - ID: nouveau UUID
```

### Cas 2 : Mise √† Jour du FE

```
Source:
  - ID: uuid-001
  - Nom: "B√©ton C35/45"
  - FE: 350.5

Input:
  - Nom: "B√©ton C35/45"
  - FE: 360.2  ‚Üê Modifi√©

R√©sultat:
  - Hash: identique
  - Match trouv√©
  - FE diff√©rent ‚Üí Operation: UPDATE
  - ID: uuid-001 (r√©utilis√©) ‚úÖ
```

### Cas 3 : M√™me Produit, Unit√© Diff√©rente

```
Source:
  - ID: uuid-002
  - Nom: "Peintures BIOPRO"
  - Unit√©: "kg"
  - FE: 8.208

Input:
  - Nom: "Peintures BIOPRO"
  - Unit√©: "m¬≤"  ‚Üê Diff√©rente
  - FE: 1.96

R√©sultat:
  - Hash: diff√©rent (unit√© dans la cl√©)
  - Aucun match
  - Operation: INSERT
  - ID: uuid-003 (nouveau) ‚úÖ
```

### Cas 4 : Doublon dans l'Import (Cas Edge)

```
Input contient 2 fois le m√™me record avec le m√™me hash mais FE diff√©rents:
  - Record A: FE=10.5, operation=UNCHANGED (vient de la source)
  - Record B: FE=12.3, operation=UPDATE (nouvel import)

D√©duplication:
  - Tri par priorit√©: UPDATE (2) avant UNCHANGED (3)
  - keep='first' apr√®s tri
  - R√©sultat: FE=12.3 gard√© ‚úÖ (version la plus r√©cente)
```

---

## üîÑ Cycle de Vie Complet

### 1. Initialisation (J0)

```
1. Import CSV ‚Üí Dataiku
2. Source vide
3. Run Python ‚Üí Tous INSERT
4. Output ‚Üí Nouvelle source
5. Upload ‚Üí Supabase staging
6. Publish ‚Üí Algolia
```

### 2. Mise √† Jour Mensuelle (J+30)

```
1. Nouveau CSV ‚Üí Dataiku
2. Source = output pr√©c√©dent
3. Run Python ‚Üí Matching actif
   - Nouveaux produits ‚Üí INSERT
   - FE modifi√©s ‚Üí UPDATE
   - Inchang√©s ‚Üí UNCHANGED
4. Output ‚Üí Nouvelle source
5. Upload ‚Üí Supabase staging
6. Publish ‚Üí Algolia (IDs stables ‚úÖ)
```

### 3. Correction Urgente (J+35)

```
1. Correction sur 1 produit
2. R√©-import du CSV
3. Run Python ‚Üí 1 UPDATE d√©tect√©
4. ID conserv√© ‚úÖ
5. Publish ‚Üí Algolia (pas de rupture)
```

---

## üìã Checklist de D√©ploiement

### Avant le Premier Run

- [ ] Dataiku projet cr√©√©
- [ ] Dataset `new_import` configur√© (CSV import)
- [ ] Dataset `emission_factors_source` cr√©√© (vide au d√©but)
- [ ] Recette Python cr√©√©e avec le code complet
- [ ] Colonnes `NATURAL_KEY` v√©rifi√©es dans le CSV

### Test sur √âchantillon

- [ ] Limite √† 1000 records
- [ ] Ex√©cution r√©ussie
- [ ] Logs : "‚úì Tous les IDs sont uniques"
- [ ] V√©rification visuelle des colonnes ID/hash/operation

### D√©ploiement Complet

- [ ] Source vid√©e (fresh start)
- [ ] Ex√©cution sur tout le dataset
- [ ] Logs finaux OK
- [ ] Output sauvegard√© comme nouvelle source

### Post-D√©ploiement

- [ ] Upload vers Supabase
- [ ] Validation SQL : 0 doublon
- [ ] Test cycle suivant (modifier quelques FE)
- [ ] V√©rifier le matching et les UPDATE

---

## üìû Support et Contact

### Si Probl√®me Persistant

1. **V√©rifier** la pr√©sence de l'unit√© dans `NATURAL_KEY` (ligne 31)
2. **V√©rifier** qu'il n'y a pas d'unit√©s vides/null dans les donn√©es
3. **V√©rifier** les logs : Message de d√©duplication ? Quel nombre ?
4. **Exporter** les doublons pour analyse :
   ```python
   dupes = df_output[df_output['ID'].duplicated(keep=False)]
   dupes.to_csv('doublons_debug.csv')
   ```

### Scripts Utiles

- **`scripts/validate_no_duplicate_ids.sql`** : Tests SQL complets
- **`scripts/analyze_natural_key_complete.py`** : Analyser la qualit√© de la cl√©
- **`scripts/extract_problematic_records.py`** : Extraire les records probl√©matiques

---

## üéâ R√©sum√©

### Ce que le Syst√®me Garantit

‚úÖ **Unicit√© absolue des IDs** (0 doublon)  
‚úÖ **Stabilit√© des IDs** entre imports  
‚úÖ **Priorit√© au nouvel import** (version la plus r√©cente)  
‚úÖ **Tra√ßabilit√©** (INSERT/UPDATE/UNCHANGED)  
‚úÖ **Int√©grit√© des donn√©es** (7 colonnes critiques v√©rifi√©es)  

### Ce que tu Obtiens

üéØ **Continuit√©** : Les m√™mes facteurs gardent leurs IDs  
üéØ **Fiabilit√©** : Impossible d'avoir des doublons d'ID  
üéØ **Fra√Æcheur** : Toujours la derni√®re version des valeurs  
üéØ **Tra√ßabilit√©** : Savoir ce qui a chang√© entre imports  

---

**Date de derni√®re mise √† jour** : 14 octobre 2025  
**Version du code** : 1.2  
**Fichier** : `dataiku_id_matching_recipe_FINAL.py`  
**Statut** : ‚úÖ Production-ready - 0 doublon garanti

