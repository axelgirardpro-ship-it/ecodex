import { ToolCallLLMMessageOptions, ToolCallLLM, LLMMetadata, LLMChatParamsStreaming, ChatResponseChunk, LLMChatParamsNonStreaming, ChatResponse } from '@llamaindex/core/llms';
import { LanguageModel, Tool } from 'ai';
import { BaseQueryEngine } from '@llamaindex/core/query-engine';
import { EngineResponse } from '@llamaindex/core/schema';

type VercelAdditionalChatOptions = ToolCallLLMMessageOptions;
declare class VercelLLM extends ToolCallLLM<VercelAdditionalChatOptions> {
    supportToolCall: boolean;
    private model;
    constructor({ model }: {
        model: LanguageModel;
    });
    get metadata(): LLMMetadata;
    private toVercelMessages;
    chat(params: LLMChatParamsStreaming<VercelAdditionalChatOptions, ToolCallLLMMessageOptions>): Promise<AsyncIterable<ChatResponseChunk<ToolCallLLMMessageOptions>>>;
    chat(params: LLMChatParamsNonStreaming<VercelAdditionalChatOptions, ToolCallLLMMessageOptions>): Promise<ChatResponse<ToolCallLLMMessageOptions>>;
}
/**
 * Convenience function to create a new VercelLLM instance.
 * @param init - initialization parameters for the VercelLLM instance.
 * @returns A new VercelLLM instance.
 */
declare const vercel: (init: ConstructorParameters<typeof VercelLLM>[0]) => VercelLLM;

interface DatasourceIndex {
    asQueryEngine: () => BaseQueryEngine;
}
type ResponseField = keyof EngineResponse;
declare function llamaindex({ model, index, description, options, }: {
    model: LanguageModel;
    index: DatasourceIndex;
    description?: string;
    options?: {
        fields?: ResponseField[];
    };
}): Tool;

export { VercelLLM, llamaindex, vercel };
export type { VercelAdditionalChatOptions };
