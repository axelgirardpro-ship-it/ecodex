# Optimisation Agent Documentaire - R√©duction Faux N√©gatifs

**Date** : 2024-10-31  
**Branche** : `optimize-chatbot-responses`  
**Commit** : 7782d7f0

## üéØ Probl√®me identifi√©

L'agent documentaire avait tendance √† r√©pondre "Je n'ai pas trouv√© d'information" √† la premi√®re question, puis trouvait la bonne r√©ponse √† une question similaire pos√©e imm√©diatement apr√®s.

### Exemple observ√©

**Question 1** : "Peux-tu m'en dire plus sur Electricit√© 2024 - mix moyen consommation dans la source Base Carbone v23.6 ?"
- ‚ùå R√©ponse : "Je n'ai pas trouv√© d'information sp√©cifique..."

**Question 2** : "comment est mod√©lis√© l'√©lectricit√© en France?"
- ‚úÖ R√©ponse correcte avec d√©tails m√©thodologiques

## üîç Causes racines identifi√©es

1. **Absence de contexte conversationnel** : Chaque question trait√©e isol√©ment
2. **R√©cup√©ration trop limit√©e** : Seulement 3 chunks de LlamaCloud
3. **Prompt trop conservateur** : Emphase excessive sur "DO NOT invent"
4. **Questions trop pr√©cises** : Termes exacts ne matchent pas toujours la documentation

## ‚úÖ Solutions impl√©ment√©es

### 1. Historique conversationnel (Frontend)

**Fichier** : `src/components/search/LlamaCloudChatModal.tsx`

```typescript
// Garder les 2 derniers √©changes (4 messages max) pour le contexte
const conversationHistory = messages.slice(-4).map(m => ({
  role: m.role,
  content: m.content
}));

body: JSON.stringify({
  message: content,
  source_name: sourceName,
  product_context: productName,
  language,
  history: conversationHistory, // ‚úÖ Ajout√©
}),
```

**Impact** : L'IA comprend le contexte et peut reformuler les questions en fonction des √©changes pr√©c√©dents.

### 2. Augmentation r√©cup√©ration chunks (Backend)

**Fichier** : `supabase/functions/llamacloud-chat-proxy/index.ts`

**Changements** :
- `similarity_top_k: 3` ‚Üí `8` (ligne 158)
- Limite sources affich√©es : `3` ‚Üí `5` (ligne 217)

```typescript
body: JSON.stringify({
  query: message,
  similarity_top_k: 8, // Augment√© de 3 √† 8
  retrieval_mode: 'chunks',
  retrieve_mode: 'text_and_images',
  filters: llamaCloudFilters
}),

// ...

const nodesToUse = allMatchingNodes.slice(0, 5); // Au lieu de 3
```

**Impact** : 
- 8 chunks r√©cup√©r√©s ‚Üí plus de chances de trouver l'info
- 5 sources affich√©es ‚Üí meilleur contexte pour l'utilisateur

### 3. Optimisation prompt syst√®me (Backend)

**Fichier** : `supabase/functions/llamacloud-chat-proxy/index.ts` (ligne 390-416)

**Avant** :
```
CRITICAL RULE - SOURCE RESTRICTION:
Answer ONLY using information from the sources above.
DO NOT invent, extrapolate, or use external knowledge.
```

**Apr√®s** :
```
SEARCH STRATEGY:
1. FIRST: Search thoroughly in the provided sources
2. If exact terms not found, look for related concepts/synonyms
3. Use conversation history to understand context
4. ONLY if truly no relevant info, suggest alternatives

DO NOT invent data or values not in the sources.
```

**Ajout section historique** :
```typescript
${history.length > 0 ? `
CONVERSATION HISTORY:
${history.map(h => `${h.role.toUpperCase()}: ${h.content}`).join('\n')}

Use this context to better understand what the user is looking for.
` : ''}
```

**Impact** : Prompt moins conservateur, encourage la recherche de concepts similaires.

### 4. Message "not found" reformul√©

**Avant** :
```
"Je n'ai pas trouv√© d'information sp√©cifique sur..."
```

**Apr√®s** :
```
"Je n'ai pas trouv√© d'information exacte sur... Voici ce que j'ai trouv√© de plus proche :"
```

**Impact** : Change la mentalit√© de "rien trouv√© = √©chec" vers "montrer ce qui s'approche".

## üìä R√©sultats attendus

- ‚úÖ **R√©duction ~60% des faux n√©gatifs**
- ‚úÖ **Meilleure compr√©hension du contexte** utilisateur
- ‚úÖ **Plus de chances de trouver l'information** pertinente (8 vs 3 chunks)
- ‚úÖ **R√©ponses plus proactives** et utiles m√™me en cas de match partiel
- ‚ö†Ô∏è **L√©g√®re augmentation co√ªts** : LlamaCloud (8 chunks vs 3) + OpenAI (tokens historique)

## üß™ Tests recommand√©s

### Tests fonctionnels

1. **Test du cas initial** :
   - Question : "Peux-tu m'en dire plus sur Electricit√© 2024 - mix moyen consommation dans la source Base Carbone v23.6 ?"
   - R√©sultat attendu : R√©ponse pertinente d√®s la premi√®re fois

2. **Test historique conversationnel** :
   - Question 1 : "Comment est mod√©lis√©e l'√©lectricit√© ?"
   - Question 2 : "Quelles sont les sources primaires utilis√©es ?"
   - R√©sultat attendu : Question 2 comprend qu'on parle toujours d'√©lectricit√©

3. **Test questions pr√©cises vs g√©n√©rales** :
   - Tester des termes exacts vs termes approchants
   - V√©rifier que l'agent trouve quand m√™me l'info

### Monitoring post-d√©ploiement

- Surveiller les co√ªts API pendant 1 semaine
- Collecter feedback utilisateurs sur qualit√© des r√©ponses
- V√©rifier logs Supabase pour taux de succ√®s

## üì¶ D√©ploiement

### Commandes

```bash
# Se connecter √† Supabase (si n√©cessaire)
npx supabase login

# D√©ployer l'Edge Function
npx supabase functions deploy llamacloud-chat-proxy

# Le frontend est d√©ploy√© automatiquement via votre CI/CD
```

### V√©rification

1. Tester l'agent dans l'interface utilisateur
2. V√©rifier les logs de l'Edge Function : `npx supabase functions logs llamacloud-chat-proxy`
3. Monitorer les m√©triques Supabase Dashboard

## üîó R√©f√©rences

- **Commit** : 7782d7f0
- **Branche** : optimize-chatbot-responses
- **Fichiers modifi√©s** :
  - `src/components/search/LlamaCloudChatModal.tsx` (17 lignes)
  - `supabase/functions/llamacloud-chat-proxy/index.ts` (10 lignes)

## üìù Notes techniques

### Limitations historique

- Maximum 4 messages (2 √©changes) pour √©viter d√©passement contexte OpenAI
- Historique non persist√© entre sessions (volontaire pour confidentialit√©)

### Co√ªts estim√©s

- **LlamaCloud** : ~2.7x plus de tokens (8 vs 3 chunks)
- **OpenAI** : +50-200 tokens par requ√™te (historique)
- **Impact estim√©** : +15-20% co√ªt par requ√™te chatbot

### Alternatives consid√©r√©es mais non retenues

1. **Re-ranking s√©mantique** : Trop complexe pour gain marginal
2. **Query expansion automatique** : Risque de d√©river du sujet
3. **Cache LlamaCloud** : Pas adapt√© √† nos patterns d'usage

